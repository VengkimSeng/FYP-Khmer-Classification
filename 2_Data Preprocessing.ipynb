{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d331e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from: https://raw.githubusercontent.com/RithDarapong/FinalYearProject/main/articles.zip\n",
      "Extracting ZIP file...\n",
      "Extracted files to /var/folders/9n/ttnnmpy11wb3xzbvs4djr1yr0000gn/T/tmpup5jb7ge/articles\n",
      "Found 6 JSON files: ['health.json', 'sport.json', 'technology.json', 'economic.json', 'politic.json', 'environment.json']\n",
      "Processing health.json with 2500 articles\n",
      "Processing sport.json with 2500 articles\n",
      "Processing technology.json with 2500 articles\n",
      "Processing economic.json with 2500 articles\n",
      "Processing politic.json with 2500 articles\n",
      "Processing environment.json with 2500 articles\n",
      "Processed 15000 articles in 22.17 seconds\n",
      "Metadata saved to metadata.csv\n",
      "Text files saved to raw_articles\n",
      "\n",
      "Metadata sample:\n",
      "   index    docId category  charCount  wordCount  \\\n",
      "0      1  health1   health       2320        846   \n",
      "1      2  health2   health       2760       1021   \n",
      "2      3  health3   health        739        267   \n",
      "3      4  health4   health       1454        541   \n",
      "4      5  health5   health       1360        495   \n",
      "\n",
      "                                                 url  \n",
      "0  https://kohsantepheapdaily.com.kh/article/1133...  \n",
      "1  https://kohsantepheapdaily.com.kh/article/1445...  \n",
      "2  https://kohsantepheapdaily.com.kh/article/2020...  \n",
      "3  https://kohsantepheapdaily.com.kh/article/1431...  \n",
      "4  https://kohsantepheapdaily.com.kh/article/%e1%...  \n",
      "\n",
      "Category statistics:\n",
      "            index  charCount              wordCount          \n",
      "            count       mean  min    max       mean min   max\n",
      "category                                                     \n",
      "economic     2500  2525.5612    0  15850   928.1688   0  5684\n",
      "environment  2500  3795.5372   14  12787  1410.1984   5  4724\n",
      "health       2500  1958.3284  158   9568   724.8360  54  3499\n",
      "politic      2500  2055.7012    0  47108   754.0160   0  7888\n",
      "sport        2500  1913.0244  284   8489   689.0028  94  3177\n",
      "technology   2500  1663.1184    0  26378   579.7176   0  8187\n"
     ]
    }
   ],
   "source": [
    "# MODULE 1: Data Loading\n",
    "# Load Data from Github\n",
    "# Convert from JSON to Txt for easy structuring and Preprocessing\n",
    "# ============================================================================\n",
    "\n",
    "# Libraries Import\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "import khmernltk\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "# Using local paths relative to current project\n",
    "TEXTS_DIR = 'raw_articles'\n",
    "METADATA_PATH = 'metadata.csv'\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs(TEXTS_DIR, exist_ok=True)\n",
    "\n",
    "def download_and_extract_github_zip():\n",
    "    \"\"\"Download the articles.zip file from GitHub and extract it\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    zip_path = os.path.join(temp_dir, \"articles.zip\")\n",
    "    \n",
    "    # Simple download using urllib\n",
    "    github_url = \"https://raw.githubusercontent.com/RithDarapong/FinalYearProject/main/articles.zip\"\n",
    "    print(f\"Downloading from: {github_url}\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(github_url, zip_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"Trying alternative URL...\")\n",
    "        alt_url = \"https://raw.githubusercontent.com/RithDarapong/FinalYearProject/main/articles.zip\"\n",
    "        urllib.request.urlretrieve(alt_url, zip_path)\n",
    "    \n",
    "    # Extract the zip file\n",
    "    print(\"Extracting ZIP file...\")\n",
    "    with zipfile.ZipFile(zip_path) as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "    \n",
    "    # Find articles directory containing JSON files\n",
    "    articles_dir = temp_dir\n",
    "    for root, dirs, files in os.walk(temp_dir):\n",
    "        if any(f.endswith('.json') for f in files):\n",
    "            articles_dir = root\n",
    "            break\n",
    "    \n",
    "    print(f\"Extracted files to {articles_dir}\")\n",
    "    return articles_dir\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count the number of words in a text.\"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "def process_articles():\n",
    "    \"\"\"Process all article JSON files and create metadata and text files.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Check if articles directory exists locally, if not download\n",
    "    if os.path.exists('articles'):\n",
    "        articles_dir = 'articles'\n",
    "        print(f\"Using local articles directory: {articles_dir}\")\n",
    "    else:\n",
    "        # Download and extract the articles from GitHub\n",
    "        articles_dir = download_and_extract_github_zip()\n",
    "    \n",
    "    # Get all JSON files in the articles directory\n",
    "    json_files = [f for f in os.listdir(articles_dir) if f.endswith('.json')]\n",
    "    \n",
    "    if not json_files:\n",
    "        raise Exception(f\"No JSON files found in {articles_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(json_files)} JSON files: {json_files}\")\n",
    "    \n",
    "    # List to store all article metadata\n",
    "    metadata = []\n",
    "    index = 1\n",
    "    \n",
    "    # Process each JSON file\n",
    "    for json_file in json_files:\n",
    "        category = os.path.splitext(json_file)[0]  # Get category from filename\n",
    "        category_counter = 1  # Initialize counter for each category\n",
    "        \n",
    "        # Load the JSON file\n",
    "        with open(os.path.join(articles_dir, json_file), 'r', encoding='utf-8') as f:\n",
    "            articles = json.load(f)\n",
    "        \n",
    "        print(f\"Processing {json_file} with {len(articles)} articles\")\n",
    "        \n",
    "        # Process each article in the JSON file\n",
    "        for article in articles:\n",
    "            # Extract required fields\n",
    "            title = article.get('title', '')\n",
    "            content = article.get('content', '')\n",
    "            url = article.get('url', '')\n",
    "            \n",
    "            # Create docId in the format {category}+{numberOrder}\n",
    "            doc_id = f\"{category}{category_counter}\"\n",
    "            category_counter += 1\n",
    "            \n",
    "            # Calculate counts\n",
    "            char_count = len(content)\n",
    "            word_count = count_words(content)\n",
    "            \n",
    "            # Create text file with title and content\n",
    "            text_path = os.path.join(TEXTS_DIR, f\"{doc_id}.txt\")\n",
    "            with open(text_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(f\"{title}\\n\\n{content}\")\n",
    "            \n",
    "            # Add to metadata\n",
    "            metadata.append({\n",
    "                'index': index,\n",
    "                'docId': doc_id,\n",
    "                'category': category,\n",
    "                'charCount': char_count,\n",
    "                'wordCount': word_count,\n",
    "                'url': url\n",
    "            })\n",
    "            \n",
    "            index += 1\n",
    "    \n",
    "    # Create metadata DataFrame and save to CSV\n",
    "    metadata_df = pd.DataFrame(metadata)\n",
    "    metadata_df.to_csv(METADATA_PATH, index=False)\n",
    "    \n",
    "    # Clean up the temporary directory only if we downloaded\n",
    "    if not os.path.exists('articles'):\n",
    "        shutil.rmtree(articles_dir)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Processed {len(metadata)} articles in {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Metadata saved to {METADATA_PATH}\")\n",
    "    print(f\"Text files saved to {TEXTS_DIR}\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "# Execute the processing function\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        metadata_df = process_articles()\n",
    "        \n",
    "        # Display the first few rows of the metadata\n",
    "        print(\"\\nMetadata sample:\")\n",
    "        print(metadata_df.head())\n",
    "        \n",
    "        # Display statistics per category\n",
    "        category_stats = metadata_df.groupby('category').agg({\n",
    "            'index': 'count',\n",
    "            'charCount': ['mean', 'min', 'max'],\n",
    "            'wordCount': ['mean', 'min', 'max']\n",
    "        })\n",
    "        print(\"\\nCategory statistics:\")\n",
    "        print(category_stats)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2c50d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15000 text files to process\n",
      "Successfully loaded 988 stopwords\n",
      "Expanded to 988 stopword variations\n",
      "Loaded and expanded 988 stopword variations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Khmer text files:   0%|          | 0/15000 [00:00<?, ?it/s]| 2025-06-14 19:26:07,396 | \u001b[1;32mINFO\u001b[0m | khmer-nltk | Loaded model from /Users/socheata/Library/Python/3.9/lib/python/site-packages/khmernltk/word_tokenize/sklearn_crf_ner_10000.sav |\n",
      "Processing Khmer text files: 100%|██████████| 15000/15000 [08:34<00:00, 29.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stopword removal statistics saved to: stopword_removal_stats.txt\n",
      "Debug information saved to: stopword_debug.txt\n",
      "Successfully processed 15000 out of 15000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# MODULE 2: Data Preprocessing\n",
    "# Removing Khmer punctuations\n",
    "# Removing special characters\n",
    "# Removing spaces (normalizing to single spaces)\n",
    "# Removing numbers (both Khmer and Arabic)\n",
    "# Perform Word Segmentation\n",
    "# Removing Stop words Removal\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import khmernltk\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Define local paths\n",
    "ORIGINAL_TEXTS_DIR = \"raw_articles\"\n",
    "PROCESSED_TEXTS_DIR = 'preprocessed_articles'\n",
    "METADATA_PATH = 'metadata.csv'\n",
    "STOPWORDS_PATH = \"Khmer-Stop-Word-1000.txt\"\n",
    "# STOPWORDS_PATH = \"khmer_stopword.txt\"\n",
    "STATS_OUTPUT_PATH = \"stopword_removal_stats.txt\"\n",
    "DEBUG_OUTPUT_PATH = \"stopword_debug.txt\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(PROCESSED_TEXTS_DIR, exist_ok=True)\n",
    "\n",
    "# Load metadata to get category information\n",
    "metadata_df = pd.read_csv(METADATA_PATH)\n",
    "# Create a dictionary for quick lookup: docId -> category\n",
    "doc_categories = dict(zip(metadata_df['docId'], metadata_df['category']))\n",
    "\n",
    "# Define Khmer character sets\n",
    "KHCONST = set(u'កខគឃងចឆជឈញដឋឌឍណតថទធនបផពភមយរលវឝឞសហឡអឣឤឥឦឧឨឩឪឫឬឭឮឯឰឱឲឳ')\n",
    "KHVOWEL = set(u'឴឵ាិីឹឺុូួើឿៀេែៃោៅ\\u17c6\\u17c7\\u17c8')\n",
    "KHSUB = set(u'្')\n",
    "KHDIAC = set(u\"\\u17c9\\u17ca\\u17cb\\u17cc\\u17cd\\u17ce\\u17cf\\u17d0\")\n",
    "KHSYM = set('៕។៛ៗ៚៙៘៖«»')\n",
    "KHNUMBER = set(u'០១២៣៤៥៦៧៨៩')\n",
    "ARABIC_NUMBER = set('0123456789')\n",
    "LATIN_CHARS = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "PUNCTUATION = set('!@#$%^&*()_+=[]{};\\'\"\\\\|,.<>?/`~፡.,፣;፤፥፦፧፪፠፨')\n",
    "\n",
    "def normalize_khmer_text(text):\n",
    "    \"\"\"Normalize Khmer text for consistent processing\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Unicode normalization\n",
    "    normalized = unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    # Remove invisible characters and control characters\n",
    "    normalized = ''.join(char for char in normalized \n",
    "                        if not unicodedata.category(char).startswith('C'))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def clean_khmer_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # First normalize the text\n",
    "    text = normalize_khmer_text(text)\n",
    "      \n",
    "    # Characters to remove: symbols, numbers, Latin chars, punctuation\n",
    "    chars_to_remove = KHSYM | KHNUMBER | ARABIC_NUMBER | LATIN_CHARS | PUNCTUATION\n",
    "    \n",
    "    # Create translation table (faster than regex for character removal)\n",
    "    translation_table = str.maketrans('', '', ''.join(chars_to_remove))\n",
    "    \n",
    "    # Apply translation to remove unwanted characters\n",
    "    text = text.translate(translation_table)\n",
    "    \n",
    "    # Normalize spaces (remove excessive spaces)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"Normalize a single word for consistent comparison\"\"\"\n",
    "    if not word:\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize unicode\n",
    "    word = unicodedata.normalize('NFC', word)\n",
    "    \n",
    "    # Remove any leading/trailing whitespace\n",
    "    word = word.strip()\n",
    "    \n",
    "    # Remove any remaining invisible characters\n",
    "    word = ''.join(char for char in word \n",
    "                   if not unicodedata.category(char).startswith('C'))\n",
    "    \n",
    "    return word\n",
    "\n",
    "def segment_khmer_text(text):\n",
    "    \"\"\"\n",
    "    Apply word segmentation to Khmer text using Khmer-NLTK\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Append title to content before segmentation\n",
    "        # Apply word segmentation using khmernltk\n",
    "        segmented_text = khmernltk.word_tokenize(text)\n",
    "        \n",
    "        # Normalize each token\n",
    "        normalized_tokens = [normalize_word(token) for token in segmented_text]\n",
    "        \n",
    "        # Filter out empty tokens\n",
    "        normalized_tokens = [token for token in normalized_tokens if token]\n",
    "        \n",
    "        # Join with spaces to create a segmented string\n",
    "        return ' '.join(normalized_tokens)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in segmentation: {e}\")\n",
    "        # If segmentation fails, return the original text\n",
    "        return text\n",
    "\n",
    "def load_khmer_stopwords(file_path):\n",
    "    \"\"\"\n",
    "    Load Khmer stopwords from a text file with normalization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize an empty list for stopwords\n",
    "        stopwords_list = []\n",
    "        \n",
    "        # Open and read the text file with UTF-8 encoding for Khmer characters\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            # Read each line and add to stopwords list\n",
    "            for line in file:\n",
    "                word = line.strip()\n",
    "                if word:  # Check if the word is not empty\n",
    "                    # Normalize the stopword\n",
    "                    normalized_word = normalize_word(word)\n",
    "                    if normalized_word:\n",
    "                        stopwords_list.append(normalized_word)\n",
    "        \n",
    "        # Create set for faster lookup\n",
    "        stopwords_set = set(stopwords_list)\n",
    "        \n",
    "        print(f\"Successfully loaded {len(stopwords_set)} stopwords\")\n",
    "        return stopwords_set\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading stopwords: {e}\")\n",
    "        # Return an empty set if loading fails\n",
    "        return set()\n",
    "\n",
    "def create_stopword_variations(stopword):\n",
    "    \"\"\"Create variations of a stopword for better matching\"\"\"\n",
    "    variations = set()\n",
    "    \n",
    "    # Add the original\n",
    "    variations.add(stopword)\n",
    "    \n",
    "    # Add different normalizations\n",
    "    variations.add(unicodedata.normalize('NFD', stopword))\n",
    "    variations.add(unicodedata.normalize('NFKC', stopword))\n",
    "    variations.add(unicodedata.normalize('NFKD', stopword))\n",
    "    \n",
    "    # Remove variations that are empty or too short\n",
    "    variations = {var for var in variations if var and len(var) > 0}\n",
    "    \n",
    "    return variations\n",
    "\n",
    "def load_khmer_stopwords_with_variations(file_path):\n",
    "    \"\"\"Load stopwords and create variations for better matching\"\"\"\n",
    "    base_stopwords = load_khmer_stopwords(file_path)\n",
    "    \n",
    "    # Create expanded set with variations\n",
    "    expanded_stopwords = set()\n",
    "    \n",
    "    for stopword in base_stopwords:\n",
    "        variations = create_stopword_variations(stopword)\n",
    "        expanded_stopwords.update(variations)\n",
    "    \n",
    "    print(f\"Expanded to {len(expanded_stopwords)} stopword variations\")\n",
    "    return expanded_stopwords\n",
    "\n",
    "def remove_stopwords(text, stopwords_set, debug_info=None):\n",
    "    \"\"\"\n",
    "    Remove stopwords from segmented Khmer text with improved matching\n",
    "    \"\"\"\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Filter out stopwords and count the removed ones\n",
    "    removed_count = 0\n",
    "    filtered_words = []\n",
    "    missed_stopwords = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Normalize the word for comparison\n",
    "        normalized_word = normalize_word(word)\n",
    "        \n",
    "        if normalized_word in stopwords_set:\n",
    "            removed_count += 1\n",
    "        else:\n",
    "            # Check if word contains any stopwords as substrings\n",
    "            found_as_substring = False\n",
    "            for stopword in stopwords_set:\n",
    "                if stopword in normalized_word or normalized_word in stopword:\n",
    "                    removed_count += 1\n",
    "                    found_as_substring = True\n",
    "                    break\n",
    "            \n",
    "            if not found_as_substring:\n",
    "                filtered_words.append(word)\n",
    "                \n",
    "                # Debug: Check if this might be a missed stopword\n",
    "                if debug_info is not None:\n",
    "                    # Check for potential stopwords based on character composition\n",
    "                    if all(char in KHCONST or char in KHVOWEL or char in KHSUB for char in normalized_word):\n",
    "                        missed_stopwords.append(word)\n",
    "    \n",
    "    if debug_info is not None:\n",
    "        debug_info['missed_stopwords'] = missed_stopwords\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    return ' '.join(filtered_words), removed_count\n",
    "\n",
    "def preprocess_text(text, stopwords_set, debug_info=None):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a text\n",
    "    \"\"\"\n",
    "    cleaned = clean_khmer_text(text)\n",
    "    segmented = segment_khmer_text(cleaned)\n",
    "    filtered, removed_count = remove_stopwords(segmented, stopwords_set, debug_info)\n",
    "    return filtered, removed_count\n",
    "\n",
    "# Main function to process all text files\n",
    "def process_khmer_text_files():\n",
    "    \"\"\"\n",
    "    Process all Khmer text files with improved stopword removal\n",
    "    \"\"\"\n",
    "    # Get list of all text files\n",
    "    text_files = [f for f in os.listdir(ORIGINAL_TEXTS_DIR) if f.endswith('.txt')]\n",
    "    print(f\"Found {len(text_files)} text files to process\")\n",
    "    \n",
    "    # Try to load stopwords with variations\n",
    "    try:\n",
    "        # Use the correct stopwords path (text file)\n",
    "        khmer_stopwords = load_khmer_stopwords_with_variations(STOPWORDS_PATH)\n",
    "        print(f\"Loaded and expanded {len(khmer_stopwords)} stopword variations\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load stopwords: {e}\")\n",
    "        khmer_stopwords = set()\n",
    "    \n",
    "    # Process each file\n",
    "    processed_count = 0\n",
    "    processing_details = []\n",
    "    debug_details = []\n",
    "    total_stopwords_removed = 0\n",
    "    \n",
    "    for filename in tqdm(text_files, desc=\"Processing Khmer text files\"):\n",
    "        try:\n",
    "            # Get docId from filename\n",
    "            doc_id = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Read the original file\n",
    "            input_path = os.path.join(ORIGINAL_TEXTS_DIR, filename)\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            \n",
    "            # Combine title and content\n",
    "            parts = text.split('\\n\\n', 1)\n",
    "            title = parts[0] if parts else \"\"\n",
    "            content = parts[1] if len(parts) > 1 else \"\"\n",
    "            \n",
    "            # Combine title and content as you mentioned\n",
    "            combined_text = f\"{title} {content}\"\n",
    "            \n",
    "            # Process the combined text with debug info\n",
    "            debug_info = {'missed_stopwords': []}\n",
    "            processed_text, stopwords_removed = preprocess_text(combined_text, khmer_stopwords, debug_info)\n",
    "            \n",
    "            total_stopwords_removed += stopwords_removed\n",
    "            \n",
    "            # Store statistics\n",
    "            processing_details.append({\n",
    "                'filename': filename,\n",
    "                'doc_id': doc_id,\n",
    "                'category': doc_categories.get(doc_id, 'unknown'),\n",
    "                'stopwords_removed': stopwords_removed\n",
    "            })\n",
    "            \n",
    "            # Store debug information\n",
    "            if debug_info['missed_stopwords']:\n",
    "                debug_details.append({\n",
    "                    'filename': filename,\n",
    "                    'missed_stopwords': debug_info['missed_stopwords'][:10]  # Limit to first 10\n",
    "                })\n",
    "            \n",
    "            # Save the processed text\n",
    "            output_path = os.path.join(PROCESSED_TEXTS_DIR, filename)\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(processed_text)\n",
    "            \n",
    "            processed_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    # Save statistics\n",
    "    save_statistics(processed_count, total_stopwords_removed, processing_details, khmer_stopwords)\n",
    "    \n",
    "    # Save debug information\n",
    "    save_debug_information(debug_details, text_files[0] if text_files else None)\n",
    "    \n",
    "    print(f\"Successfully processed {processed_count} out of {len(text_files)} files\")\n",
    "\n",
    "def save_statistics(processed_count, total_stopwords_removed, processing_details, khmer_stopwords):\n",
    "    \"\"\"Save stopword removal statistics to a text file\"\"\"\n",
    "    with open(STATS_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"================================\\n\")\n",
    "        f.write(\"Khmer Text Preprocessing Statistics\\n\")\n",
    "        f.write(\"================================\\n\\n\")\n",
    "        f.write(f\"Total files processed: {processed_count}\\n\")\n",
    "        f.write(f\"Total stopwords loaded (with variations): {len(khmer_stopwords)}\\n\")\n",
    "        f.write(f\"Total stopwords removed across all files: {total_stopwords_removed}\\n\")\n",
    "        f.write(f\"Average stopwords removed per file: {total_stopwords_removed/processed_count if processed_count > 0 else 0:.2f}\\n\")\n",
    "        f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "        f.write(\"Per-file Details:\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "        \n",
    "        for detail in processing_details:\n",
    "            f.write(f\"File: {detail['filename']} (ID: {detail['doc_id']}, Category: {detail['category']})\\n\")\n",
    "            f.write(f\"  Stopwords removed: {detail['stopwords_removed']}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    print(f\"\\nStopword removal statistics saved to: {STATS_OUTPUT_PATH}\")\n",
    "\n",
    "def save_debug_information(debug_details, sample_file):\n",
    "    \"\"\"Save debug information about potentially missed stopwords\"\"\"\n",
    "    with open(DEBUG_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=====================================\\n\")\n",
    "        f.write(\"Debug: Potentially Missed Stopwords\\n\")\n",
    "        f.write(\"=====================================\\n\\n\")\n",
    "        \n",
    "        for detail in debug_details[:10]:  # Limit to first 10 files for clarity\n",
    "            f.write(f\"File: {detail['filename']}\\n\")\n",
    "            f.write(\"Potentially missed stopwords:\\n\")\n",
    "            for i, word in enumerate(detail['missed_stopwords'][:10], 1):\n",
    "                f.write(f\"  {i}. '{word}'\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "        \n",
    "        # Show sample processing\n",
    "        if sample_file:\n",
    "            f.write(\"\\n\\nSAMPLE PROCESSING EXAMPLE\\n\")\n",
    "            f.write(\"========================\\n\")\n",
    "            \n",
    "            try:\n",
    "                with open(os.path.join(ORIGINAL_TEXTS_DIR, sample_file), 'r', encoding='utf-8') as original:\n",
    "                    original_text = original.read()\n",
    "                \n",
    "                with open(os.path.join(PROCESSED_TEXTS_DIR, sample_file), 'r', encoding='utf-8') as processed:\n",
    "                    processed_text = processed.read()\n",
    "                \n",
    "                f.write(f\"File: {sample_file}\\n\")\n",
    "                f.write(\"\\nOriginal words (first 50):\\n\")\n",
    "                original_words = original_text.split()[:50]\n",
    "                f.write(' '.join(original_words) + \"\\n\")\n",
    "                \n",
    "                f.write(\"\\nProcessed words (first 50):\\n\")\n",
    "                processed_words = processed_text.split()[:50]\n",
    "                f.write(' '.join(processed_words) + \"\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                f.write(f\"Error showing sample: {e}\\n\")\n",
    "    \n",
    "    print(f\"Debug information saved to: {DEBUG_OUTPUT_PATH}\")\n",
    "\n",
    "# Run the processing function\n",
    "if __name__ == \"__main__\":\n",
    "    process_khmer_text_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
